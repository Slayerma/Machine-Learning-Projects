{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import re\n",
    "import logging\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from configs import CONFIGS  # noqa\n",
    "from src.dataset import DatasetRetriever  # noqa\n",
    "from src.ctc_labeling import CTCLabeling  # noqa\n",
    "from src.model import get_ocr_model  # noqa\n",
    "import src.utils as utils  # noqa\n",
    "from src.predictor import Predictor  # noqa\n",
    "from src.metrics import string_accuracy, cer, wer  # noqa\n",
    "\n",
    "def as_asr_text(text: str) -> str:\n",
    "    \"\"\" Lowercase and remove punctuation from text \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def main(args: argparse.Namespace) -> None:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    assert args.dataset_name in CONFIGS, f\"Invalid dataset_name: {args.dataset_name}\"\n",
    "\n",
    "    utils.seed_everything(args.seed)\n",
    "\n",
    "    config = CONFIGS[args.dataset_name](\n",
    "        data_dir=args.data_dir,\n",
    "        image_w=args.image_w,\n",
    "        image_h=args.image_h,\n",
    "        bs=args.bs,\n",
    "        num_workers=args.num_workers,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    logging.info(f'DEVICE: {device}')\n",
    "    logging.info(f'DATASET: {args.dataset_name}')\n",
    "\n",
    "    ctc_labeling = CTCLabeling(config)\n",
    "\n",
    "    df = pd.read_csv(args.data_dir / f'{args.dataset_name}/marking.csv', index_col='sample_id')\n",
    "\n",
    "    valid_dataset = DatasetRetriever(df[df['stage'] == 'valid'], config, ctc_labeling)\n",
    "    test_dataset = DatasetRetriever(df[df['stage'] == 'test'], config, ctc_labeling)\n",
    "\n",
    "    model = get_ocr_model(config, pretrained=False)\n",
    "    model = model.to(device)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config['bs'],\n",
    "        sampler=SequentialSampler(valid_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        collate_fn=utils.kw_collate_fn\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['bs'],\n",
    "        sampler=SequentialSampler(test_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        collate_fn=utils.kw_collate_fn\n",
    "    )\n",
    "\n",
    "    result_metrics: List[Dict[str, float]] = []\n",
    "    for experiment_folder in glob(str(args.experiment_folder / '*')):\n",
    "        logging.info(experiment_folder)\n",
    "        exp_metrics = defaultdict(list)\n",
    "        for checkpoint_path in glob(f'{experiment_folder}/*.pt'):\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "\n",
    "            predictor = Predictor(model, device)\n",
    "            valid_predictions = predictor.run_inference(valid_loader)\n",
    "            test_predictions = predictor.run_inference(test_loader)\n",
    "\n",
    "            df_valid_pred = pd.DataFrame([{\n",
    "                'id': prediction['id'],\n",
    "                'pred_text': ctc_labeling.decode(prediction['raw_output'].argmax(1)),\n",
    "                'gt_text': prediction['gt_text']\n",
    "            } for prediction in valid_predictions]).set_index('id')\n",
    "            df_test_pred = pd.DataFrame([{\n",
    "                'id': prediction['id'],\n",
    "                'pred_text': ctc_labeling.decode(prediction['raw_output'].argmax(1)),\n",
    "                'gt_text': prediction['gt_text']\n",
    "            } for prediction in test_predictions]).set_index('id')\n",
    "\n",
    "            exp_metrics['cer_valid'].append(round(cer(df_valid_pred['pred_text'], df_valid_pred['gt_text']), 5))\n",
    "            exp_metrics['wer_valid'].append(round(wer(df_valid_pred['pred_text'], df_valid_pred['gt_text']), 5))\n",
    "            exp_metrics['acc_valid'].append(round(\n",
    "                string_accuracy(df_valid_pred['pred_text'], df_valid_pred['gt_text']), 5))\n",
    "\n",
    "            exp_metrics['cer_test'].append(round(cer(df_test_pred['pred_text'], df_test_pred['gt_text']), 5))\n",
    "            exp_metrics['wer_test'].append(round(wer(df_test_pred['pred_text'], df_test_pred['gt_text']), 5))\n",
    "            exp_metrics['acc_test'].append(round(\n",
    "                string_accuracy(df_test_pred['pred_text'], df_test_pred['gt_text']), 5))\n",
    "\n",
    "            df_valid_pred['pred_text'] = df_valid_pred['pred_text'].apply(as_asr_text)\n",
    "            df_valid_pred['gt_text'] = df_valid_pred['gt_text'].apply(as_asr_text)\n",
    "            df_test_pred['pred_text'] = df_test_pred['pred_text'].apply(as_asr_text)\n",
    "            df_test_pred['gt_text'] = df_test_pred['gt_text'].apply(as_asr_text)\n",
    "\n",
    "            exp_metrics['cer_valid_asr'].append(round(cer(df_valid_pred['pred_text'], df_valid_pred['gt_text']), 5))\n",
    "            exp_metrics['wer_valid_asr'].append(round(wer(df_valid_pred['pred_text'], df_valid_pred['gt_text']), 5))\n",
    "            exp_metrics['acc_valid_asr'].append(round(\n",
    "                string_accuracy(df_valid_pred['pred_text'], df_valid_pred['gt_text']), 5))\n",
    "\n",
    "            exp_metrics['cer_test_asr'].append(round(cer(df_test_pred['pred_text'], df_test_pred['gt_text']), 5))\n",
    "            exp_metrics['wer_test_asr'].append(round(wer(df_test_pred['pred_text'], df_test_pred['gt_text']), 5))\n",
    "            exp_metrics['acc_test_asr'].append(round(\n",
    "                string_accuracy(df_test_pred['pred_text'], df_test_pred['gt_text']), 5))\n",
    "\n",
    "        result_metrics.append({\n",
    "            'cer_valid': min(exp_metrics['cer_valid']),\n",
    "            'wer_valid': min(exp_metrics['wer_valid']),\n",
    "            'acc_valid': max(exp_metrics['acc_valid']),\n",
    "            'cer_test': min(exp_metrics['cer_test']),\n",
    "            'wer_test': min(exp_metrics['wer_test']),\n",
    "            'acc_test': max(exp_metrics['acc_test']),\n",
    "            'cer_valid_asr': min(exp_metrics['cer_valid_asr']),\n",
    "            'wer_valid_asr': min(exp_metrics['wer_valid_asr']),\n",
    "            'acc_valid_asr': max(exp_metrics['acc_valid_asr']),\n",
    "            'cer_test_asr': min(exp_metrics['cer_test_asr']),\n",
    "            'wer_test_asr': min(exp_metrics['wer_test_asr']),\n",
    "            'acc_test_asr': max(exp_metrics['acc_test_asr']),\n",
    "        })\n",
    "\n",
    "        logging.info('---- VALID ----')\n",
    "        logging.info(f'CER: {min(exp_metrics[\"cer_valid\"])}')\n",
    "        logging.info(f'WER: {min(exp_metrics[\"wer_valid\"])}')\n",
    "        logging.info(f'ACC: {max(exp_metrics[\"acc_valid\"])}')\n",
    "        logging.info('---- TEST -----')\n",
    "        logging.info(f'CER: {min(exp_metrics[\"cer_test\"])}')\n",
    "        logging.info(f'WER: {min(exp_metrics[\"wer_test\"])}')\n",
    "        logging.info(f'ACC: {max(exp_metrics[\"acc_test\"])}')\n",
    "        logging.info('---- VALID as ASR ----')\n",
    "        logging.info(f'CER: {min(exp_metrics[\"cer_valid_asr\"])}')\n",
    "        logging.info(f'WER: {min(exp_metrics[\"wer_valid_asr\"])}')\n",
    "        logging.info(f'ACC: {max(exp_metrics[\"acc_valid_asr\"])}')\n",
    "        logging.info('---- TEST as ASR -----')\n",
    "        logging.info(f'CER: {min(exp_metrics[\"cer_test_asr\"])}')\n",
    "        logging.info(f'WER: {min(exp_metrics[\"wer_test_asr\"])}')\n",
    "        logging.info(f'ACC: {max(exp_metrics[\"acc_test_asr\"])}')\n",
    "\n",
    "    result_metrics = pd.DataFrame(result_metrics)\n",
    "\n",
    "    def mean_std(key: str, ndigits: int = 4) -> str:\n",
    "        mean = round(result_metrics[key].mean(), ndigits=ndigits)\n",
    "        std = round(result_metrics[key].std(), ndigits=ndigits)\n",
    "        return f'{mean} Â± {std} [{round(std / mean * 100, 1)}%]'\n",
    "\n",
    "    logging.info('---- ----- ----')\n",
    "    logging.info('---- VALID ----')\n",
    "    logging.info(f'CER: {mean_std(\"cer_valid\")}')\n",
    "    logging.info(f'WER: {mean_std(\"wer_valid\")}')\n",
    "    logging.info(f'ACC: {mean_std(\"acc_valid\")}')\n",
    "    logging.info('---- TEST -----')\n",
    "    logging.info(f'CER: {mean_std(\"cer_test\")}')\n",
    "    logging.info(f'WER: {mean_std(\"wer_test\")}')\n",
    "    logging.info(f'ACC: {mean_std(\"acc_test\")}')\n",
    "    logging.info('---- VALID as ASR ----')\n",
    "    logging.info(f'CER: {mean_std(\"cer_valid_asr\")}')\n",
    "    logging.info(f'WER: {mean_std(\"wer_valid_asr\")}')\n",
    "    logging.info(f'ACC: {mean_std(\"acc_valid_asr\")}')\n",
    "    logging.info('---- TEST as ASR -----')\n",
    "    logging.info(f'CER: {mean_std(\"cer_test_asr\")}')\n",
    "    logging.info(f'WER: {mean_std(\"wer_test_asr\")}')\n",
    "    logging.info(f'ACC: {mean_std(\"acc_test_asr\")}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Run evaluation script.')\n",
    "    parser.add_argument('--experiment_folder', type=Path, required=True, help='Path to the experiment folder')\n",
    "    parser.add_argument('--dataset_name', type=str, required=True, help='Name of the dataset')\n",
    "    parser.add_argument('--image_w', type=int, required=True, help='Image width')\n",
    "    parser.add_argument('--image_h', type=int, required=True, help='Image height')\n",
    "    parser.add_argument('--data_dir', type=Path, default=Path('../StackMix-OCR-DATA'), help='Path to the data directory')\n",
    "    parser.add_argument('--bs', type=int, default=16, help='Batch size')\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='Number of workers for data loading')\n",
    "    parser.add_argument('--seed', type=int, default=6955, help='Random seed')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import albumentations as A\n",
    "import neptune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from configs import CONFIGS  # noqa\n",
    "from src.dataset import DatasetRetriever  # noqa\n",
    "from src.ctc_labeling import CTCLabeling  # noqa\n",
    "from src.model import get_ocr_model  # noqa\n",
    "from src.experiment import OCRExperiment  # noqa\n",
    "import src.utils as utils  # noqa\n",
    "from src.predictor import Predictor  # noqa\n",
    "from src.blot import get_blot_transforms  # noqa\n",
    "from src.metrics import string_accuracy, cer, wer  # noqa\n",
    "from src.stackmix import StackMix  # noqa\n",
    "\n",
    "def get_transforms(config: Dict, use_augs: bool, use_blot: bool) -> Optional[A.Compose]:\n",
    "    if use_blot and use_augs:\n",
    "        return A.Compose([\n",
    "            get_blot_transforms(config),\n",
    "            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "            A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),\n",
    "            A.JpegCompression(quality_lower=75, p=0.5),\n",
    "        ], p=1.0)\n",
    "    elif use_blot:\n",
    "        return get_blot_transforms(config)\n",
    "    elif use_augs:\n",
    "        return A.Compose([\n",
    "            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "            A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),\n",
    "            A.JpegCompression(quality_lower=75, p=0.5),\n",
    "        ], p=1.0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def main(args: argparse.Namespace) -> None:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    assert args.dataset_name in CONFIGS, f\"Invalid dataset_name: {args.dataset_name}\"\n",
    "\n",
    "    if args.checkpoint_path:\n",
    "        seed = round(datetime.utcnow().timestamp()) % 10000  # warning! in resume need change seed\n",
    "    else:\n",
    "        seed = args.seed\n",
    "\n",
    "    utils.seed_everything(seed)\n",
    "\n",
    "    config = CONFIGS[args.dataset_name](\n",
    "        data_dir=args.data_dir,\n",
    "        experiment_name=args.experiment_name,\n",
    "        experiment_description=args.experiment_description,\n",
    "        image_w=args.image_w,\n",
    "        image_h=args.image_h,\n",
    "        num_epochs=args.num_epochs,\n",
    "        bs=args.bs,\n",
    "        num_workers=args.num_workers,\n",
    "        seed=seed,\n",
    "        use_blot=args.use_blot,\n",
    "        use_augs=args.use_augs,\n",
    "        use_stackmix=args.use_stackmix,\n",
    "        use_pretrained_backbone=args.use_pretrained_backbone,\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    logging.info(f'DEVICE: {device}')\n",
    "    logging.info(f'DATASET: {args.dataset_name}')\n",
    "\n",
    "    ctc_labeling = CTCLabeling(config)\n",
    "\n",
    "    df = pd.read_csv(args.data_dir / f'{args.dataset_name}/marking.csv', index_col='sample_id')\n",
    "\n",
    "    transforms = get_transforms(config, args.use_augs, args.use_blot)\n",
    "\n",
    "    train_dataset_kwargs = {'transforms': transforms}\n",
    "    if args.use_stackmix:\n",
    "        stackmix = StackMix(\n",
    "            mwe_tokens_dir=args.mwe_tokens_dir,\n",
    "            data_dir=args.data_dir,\n",
    "            dataset_name=args.dataset_name,\n",
    "            image_h=args.image_h,\n",
    "        )\n",
    "        stackmix.load()\n",
    "        stackmix.load_corpus(ctc_labeling, args.data_dir / f'corpora/{config.corpus_name}')\n",
    "        train_dataset_kwargs['stackmix'] = stackmix\n",
    "\n",
    "    df_train = df[~df['stage'].isin(['valid', 'test'])]\n",
    "    train_dataset = DatasetRetriever(df_train, config, ctc_labeling, **train_dataset_kwargs)\n",
    "    valid_dataset = DatasetRetriever(df[df['stage'] == 'valid'], config, ctc_labeling)\n",
    "    test_dataset = DatasetRetriever(df[df['stage'] == 'test'], config, ctc_labeling)\n",
    "\n",
    "    model = get_ocr_model(config, pretrained=bool(args.use_pretrained_backbone))\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), **config['optimizer']['params'])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['bs'],\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        collate_fn=utils.kw_collate_fn\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config['bs'],\n",
    "        sampler=SequentialSampler(valid_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        collate_fn=utils.kw_collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['bs'],\n",
    "        sampler=SequentialSampler(test_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        collate_fn=utils.kw_collate_fn\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        epochs=config['num_epochs'],\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        **config['scheduler']['params'],\n",
    "    )\n",
    "\n",
    "    neptune_kwargs = {}\n",
    "    if args.neptune_project:\n",
    "        tags = [args.dataset_name]\n",
    "        if args.use_blot and args.use_stackmix and args.use_augs:\n",
    "            tags.append('blots_augs_stackmix')\n",
    "        elif args.use_blot and args.use_stackmix:\n",
    "            tags.append('blots_stackmix')\n",
    "        elif args.use_blot and args.use_augs:\n",
    "            tags.append('blots_augs')\n",
    "        elif args.use_stackmix and args.use_augs:\n",
    "            tags.append('augs_stackmix')\n",
    "        elif args.use_stackmix:\n",
    "            tags.append('stackmix')\n",
    "        elif args.use_blot:\n",
    "            tags.append('blots')\n",
    "        elif args.use_augs:\n",
    "            tags.append('augs')\n",
    "        else:\n",
    "            tags.append('base')\n",
    "\n",
    "        neptune.init(\n",
    "            project_qualified_name=args.neptune_project,\n",
    "            api_token=args.neptune_token,\n",
    "        )\n",
    "        neptune_kwargs = dict(\n",
    "            neptune=neptune,\n",
    "            neptune_params={\n",
    "                'description': config['experiment_description'],\n",
    "                'params': config.params,\n",
    "                'tags': tags,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not args.checkpoint_path:\n",
    "        experiment = OCRExperiment(\n",
    "            experiment_name=config['experiment_name'],\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            base_dir=args.output_dir,\n",
    "            best_saving={'cer': 'min', 'wer': 'min', 'acc': 'max'},\n",
    "            last_saving=True,\n",
    "            low_memory=True,\n",
    "            verbose_step=10**5,\n",
    "            seed=seed,\n",
    "            use_progress_bar=bool(args.use_progress_bar),\n",
    "            **neptune_kwargs,\n",
    "            ctc_labeling=ctc_labeling,\n",
    "        )\n",
    "        experiment.fit(train_loader, valid_loader, config['num_epochs'])\n",
    "    else:\n",
    "        logging.info(f'RESUMED FROM: {args.checkpoint_path}')\n",
    "        experiment = OCRExperiment.resume(\n",
    "            checkpoint_path=args.checkpoint_path,\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            n_epochs=config['num_epochs'],\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "            neptune=neptune_kwargs.get('neptune'),\n",
    "            ctc_labeling=ctc_labeling,\n",
    "        )\n",
    "\n",
    "    time_inference = []\n",
    "    for best_metric in ['best_cer', 'best_wer', 'best_acc', 'last']:\n",
    "        experiment.load(experiment.experiment_dir / f'{best_metric}.pt')\n",
    "        experiment.model.eval()\n",
    "        predictor = Predictor(experiment.model, device)\n",
    "        time_a = time.time()\n",
    "        predictions = predictor.run_inference(test_loader)\n",
    "        time_b = time.time()\n",
    "        time_inference.append(time_b - time_a)\n",
    "        df_pred = pd.DataFrame([{\n",
    "            'id': prediction['id'],\n",
    "            'pred_text': ctc_labeling.decode(prediction['raw_output'].argmax(1)),\n",
    "            'gt_text': prediction['gt_text']\n",
    "        } for prediction in predictions]).set_index('id')\n",
    "\n",
    "        cer_metric = round(cer(df_pred['pred_text'], df_pred['gt_text']), 5)\n",
    "        wer_metric = round(wer(df_pred['pred_text'], df_pred['gt_text']), 5)\n",
    "        acc_metric = round(string_accuracy(df_pred['pred_text'], df_pred['gt_text']), 5)\n",
    "\n",
    "        if args.neptune_project:\n",
    "            experiment.neptune.log_metric(f'cer_test__{best_metric}', cer_metric)\n",
    "            experiment.neptune.log_metric(f'wer_test__{best_metric}', wer_metric)\n",
    "            experiment.neptune.log_metric(f'acc_test__{best_metric}', acc_metric)\n",
    "\n",
    "        mistakes = df_pred[df_pred['pred_text'] != df_pred['gt_text']]\n",
    "        df_pred.to_csv(experiment.experiment_dir / f'pred__{best_metric}.csv')\n",
    "\n",
    "        if args.neptune_project:\n",
    "            experiment.neptune.log_metric(f'mistakes__{best_metric}', mistakes.shape[0])\n",
    "            experiment.neptune.log_artifact(str(experiment.experiment_dir / f'pred__{best_metric}.csv'))\n",
    "\n",
    "        experiment._log(  # noqa\n",
    "            f'Results for {best_metric}.pt.',\n",
    "            cer=cer_metric,\n",
    "            wer=wer_metric,\n",
    "            acc=acc_metric,\n",
    "            speed_inference=len(test_dataset) / (time_b - time_a),\n",
    "        )\n",
    "\n",
    "    if args.neptune_project:\n",
    "        experiment.neptune.log_metric('time_inference', np.mean(time_inference))\n",
    "        experiment.neptune.log_metric('speed_inference', len(test_dataset) / np.mean(time_inference))  # sample / sec\n",
    "        experiment.neptune.log_metric('train_per_epoch_iterations', len(train_loader))\n",
    "\n",
    "    experiment.destroy()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Run train script.')\n",
    "    parser.add_argument('--checkpoint_path', type=Path, default=None, help='Path to the checkpoint file')\n",
    "    parser.add_argument('--experiment_name', type=str, required=True, help='Name of the experiment')\n",
    "    parser.add_argument('--use_augs', type=int, required=True, help='Whether to use augmentations')\n",
    "    parser.add_argument('--use_blot', type=int, required=True, help='Whether to use blot augmentations')\n",
    "    parser.add_argument('--use_stackmix', type=int, required=True, help='Whether to use StackMix augmentations')\n",
    "    parser.add_argument('--neptune_project', type=str, default=None, help='Neptune project name')\n",
    "    parser.add_argument('--neptune_token', type=str, default=None, help='Neptune token')\n",
    "    parser.add_argument('--data_dir', type=Path, required=True, help='Path to the data directory')\n",
    "    parser.add_argument('--mwe_tokens_dir', type=Path, required=True, help='Path to the MWE tokens directory')\n",
    "    parser.add_argument('--output_dir', type=Path, required=True, help='Path to the output directory')\n",
    "    parser.add_argument('--experiment_description', type=str, required=True, help='Description of the experiment')\n",
    "    parser.add_argument('--dataset_name', type=str, required=True, help='Name of the dataset')\n",
    "    parser.add_argument('--image_w', type=int, required=True, help='Image width')\n",
    "    parser.add_argument('--image_h', type=int, required=True, help='Image height')\n",
    "    parser.add_argument('--num_epochs', type=int, required=True, help='Number of epochs')\n",
    "    parser.add_argument('--bs', type=int, required=True, help='Batch size')\n",
    "    parser.add_argument('--num_workers', type=int, required=True, help='Number of workers for data loading')\n",
    "    parser.add_argument('--seed', type=int, default=6955, help='Random seed')\n",
    "    parser.add_argument('--use_progress_bar', type=int, default=0, help='Whether to use progress bar')\n",
    "    parser.add_argument('--use_pretrained_backbone', type=int, default=1, help='Whether to use pretrained backbone')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "# Text to be rendered\n",
    "txt = \"\"\"Python is an interpreted high-level general-purpose programming\n",
    "Its design philosophy emphasizes code readability with its use of signi\"\"\"\n",
    "\n",
    "# Create a new image with a white background\n",
    "width, height = 800, 600\n",
    "image = Image.new('RGB', (width, height), color=(255, 255, 255))\n",
    "\n",
    "# Create a drawing object\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Use a built-in font provided by PIL\n",
    "# font = ImageFont.truetype(font_path, 24)\n",
    "font = ImageFont.truetype('wg_goodbye_font.ttf', 24)  # Replace 'Arial.ttf' with any built-in font name\n",
    "\n",
    "# Split the text into lines based on a maximum line width\n",
    "max_line_width = 50\n",
    "lines = textwrap.wrap(txt, max_line_width)\n",
    "\n",
    "# Draw the lines of text onto the image\n",
    "y = 10\n",
    "for line in lines:\n",
    "    draw.text((10, y), line, font=font, fill=(0, 0, 138))\n",
    "    y += font.size + 10\n",
    "\n",
    "# Save the image\n",
    "image.save(\"handwritten_text.png\")\n",
    "print(\"END\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
